# 🧠 Research Assistant using LangChain + Hugging Face + FAISS

This project is an interactive research assistant that allows users to enter URLs of articles, ask questions, and get answers generated by a local Hugging Face language model.

### 🚀 Features

- ✅ Load web content via URL
- ✂️ Smart text splitting using LangChain's `RecursiveCharacterTextSplitter`
- 🔍 Embedding with Hugging Face (`all-MiniLM-L6-v2`)
- 🧠 Semantic Search using FAISS vector store
- 💬 Question answering with a local LLM (e.g., `google/flan-t5-small`)
- 🌐 Simple web interface with Streamlit
- 🗃️ Vector DB persistence using `pickle`

### 🛠️ Tech Stack

- `LangChain` for RAG pipeline
- `Hugging Face Transformers` for embeddings & LLM
- `FAISS` for vector similarity search
- `Streamlit` for the user interface

### 📦 How It Works

1. User enters up to 3 article URLs
2. Content is loaded and chunked
3. Chunks are embedded and stored in FAISS
4. User types a question
5. Top relevant chunks are retrieved
6. Answer is generated using Hugging Face LLM
7. Sources are displayed for transparency

### 📁 File Overview

- `main.py` – Main Streamlit app
- `faiss_store_hf.pkl` – Pickled FAISS vector store

### 🧪 Example Models Used

- Embeddings: `all-MiniLM-L6-v2`
- LLM: `google/flan-t5-small` (can be swapped for `mistral`, `falcon`, etc.)

### ⚙️ Setup Instructions

```bash
pip install -r requirements.txt
streamlit run main.py
