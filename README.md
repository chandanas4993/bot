# ğŸ§  Research Assistant using LangChain + Hugging Face + FAISS

This project is an interactive research assistant that allows users to enter URLs of articles, ask questions, and get answers generated by a local Hugging Face language model.

### ğŸš€ Features

- âœ… Load web content via URL
- âœ‚ï¸ Smart text splitting using LangChain's `RecursiveCharacterTextSplitter`
- ğŸ” Embedding with Hugging Face (`all-MiniLM-L6-v2`)
- ğŸ§  Semantic Search using FAISS vector store
- ğŸ’¬ Question answering with a local LLM (e.g., `google/flan-t5-small`)
- ğŸŒ Simple web interface with Streamlit
- ğŸ—ƒï¸ Vector DB persistence using `pickle`

### ğŸ› ï¸ Tech Stack

- `LangChain` for RAG pipeline
- `Hugging Face Transformers` for embeddings & LLM
- `FAISS` for vector similarity search
- `Streamlit` for the user interface

### ğŸ“¦ How It Works

1. User enters up to 3 article URLs
2. Content is loaded and chunked
3. Chunks are embedded and stored in FAISS
4. User types a question
5. Top relevant chunks are retrieved
6. Answer is generated using Hugging Face LLM
7. Sources are displayed for transparency

### ğŸ“ File Overview

- `main.py` â€“ Main Streamlit app
- `faiss_store_hf.pkl` â€“ Pickled FAISS vector store

### ğŸ§ª Example Models Used

- Embeddings: `all-MiniLM-L6-v2`
- LLM: `google/flan-t5-small` (can be swapped for `mistral`, `falcon`, etc.)

### âš™ï¸ Setup Instructions

```bash
pip install -r requirements.txt
streamlit run main.py
